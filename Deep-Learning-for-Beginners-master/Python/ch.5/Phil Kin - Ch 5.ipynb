{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad75254",
   "metadata": {},
   "source": [
    "# Vanishing Gradient\n",
    "- Gradient pode ser interpretado como o delta do Back-Propagation\n",
    "- Como o Back-Propagation treina os nós de trás para frente, como o erro difícilmente atinge as primeiras camadas, acaba que o peso delas não é ajustado, sendo as camadas mais próximas da entrada não treinadas.\n",
    "- A solução apresentada pela Vanishing Gradient é a introdução da Rectified Linear Unit (ReLu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0805c678",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "- O modelo se torna complicado a cada nova camada oculta.\n",
    "- A solução é treinar apenas alguns nós selecionados aleatóriamente ao invés da rede toda, dessa forma evita sobreajustes.\n",
    "- Além disso, o uso de dados massivos de treinamento também é muito útil, pois o viés potencial devido a dados específicos é reduzido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9097cd1c",
   "metadata": {},
   "source": [
    "# Computational Load\n",
    "- Esse tópico trata o tempo de processamento e aprendizado da rede. Sendo que o número de pessos aumenta geometricamente com o número de camadas ocultas.\n",
    "- Este problema foi aliviado em grande medida pelo introdução de hardware de alto desempenho, como GPU, e algoritmos, como normalização em lote."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c09d9f9",
   "metadata": {},
   "source": [
    "- O Deep Learning supera todas as técnicas de todas as 3 áreas tratadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7967e7d",
   "metadata": {},
   "source": [
    "# Example: ReLU and Dropout\n",
    "\n",
    "## ReLU Function\n",
    "- A função pega os pesos da rede e os dados de entrada e retorna os pesos treinados\n",
    "- A função tem falhas e pode produzir saídas erradas, diferente da função sigmoid. A sensibilidade da função ReLU aos valores de peso iniciais parece causar esta anomalia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b97765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y =  1 : \n",
      "[[9.99962223e-01]\n",
      " [1.30664794e-05]\n",
      " [7.90549956e-08]\n",
      " [9.75380210e-06]\n",
      " [1.48781526e-05]]\n",
      "Y =  2 : \n",
      "[[4.41781717e-09]\n",
      " [9.99987952e-01]\n",
      " [1.20433276e-05]\n",
      " [8.17658706e-12]\n",
      " [6.25992992e-12]]\n",
      "Y =  3 : \n",
      "[[1.65354489e-06]\n",
      " [6.09878214e-06]\n",
      " [9.99982471e-01]\n",
      " [9.77581688e-06]\n",
      " [5.00957382e-10]]\n",
      "Y =  4 : \n",
      "[[2.58342885e-05]\n",
      " [1.05541912e-10]\n",
      " [1.67865852e-08]\n",
      " [9.99974148e-01]\n",
      " [1.21419462e-09]]\n",
      "Y =  5 : \n",
      "[[8.71823735e-06]\n",
      " [1.11371330e-06]\n",
      " [2.76665499e-08]\n",
      " [3.83445151e-11]\n",
      " [9.99990140e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Implementaçao da função sigmoid\n",
    "def Sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "#Implementaçao da função softmax\n",
    "def Softmax(x):\n",
    "    x  = np.subtract(x, np.max(x))\n",
    "    ex = np.exp(x)\n",
    "    \n",
    "    return ex / np.sum(ex)\n",
    "\n",
    "#Implementaçao da função ReLu\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "#Cálculo dos pesos pela função ReLu\n",
    "def DeepReLU(W1, W2, W3, W4, X, D):\n",
    "    \n",
    "    #Taxa de aprendizado\n",
    "    alpha = 0.01\n",
    "    \n",
    "    N = 5\n",
    "    #Percorre as matrizes de teste\n",
    "    for k in range(N):\n",
    "        x  = np.reshape(X[:, :, k], (25, 1)) #Transforma a matiz em um vetor 25x1\n",
    "        d = D[k, :].T #Tanspoem a matriz de resultados\n",
    "        \n",
    "        #Primeira camada\n",
    "        v1 = np.matmul(W1, x) #Multiplica W1 (peso) com o array de teste\n",
    "        y1 = ReLU(v1) #Calcula a saída do neurônio (função de ativação ReLu)\n",
    "        \n",
    "        #Segunda camada\n",
    "        v2 = np.matmul(W2, y1) #Multiplica W2 (peso) com o array gerado pelo primeiro neurônio\n",
    "        y2 = ReLU(v2) #Calcula a saída do neurônio (função de ativação ReLu)\n",
    "        \n",
    "        #Terceira camada\n",
    "        v3 = np.matmul(W3, y2) #Multiplica W3 (peso) com o array gerado pelo segundo neurônio\n",
    "        y3 = ReLU(v3) #Calcula a saída do neurônio (função de ativação ReLu)\n",
    "        \n",
    "        #Quarta camada\n",
    "        v = np.matmul(W4, y3) #Multiplica W4 (peso) com o array gerado pelo terceiro neurônio\n",
    "        y = Softmax(v) #Calcula a saída do neurônio (função de ativação softmax)\n",
    "        \n",
    "        #Quarto neurônio\n",
    "        e = d - y #Calcula o erro\n",
    "        delta = e #Calculado o delta em cima do erro\n",
    "        \n",
    "        #Terceiro neurônio\n",
    "        e3 = np.matmul(W4.T, delta) #Calcula o erro\n",
    "        delta3 = (v3 > 0) * e3 #Calculado o delta em cima do erro\n",
    "        \n",
    "        #Segundo neurônio\n",
    "        e2 = np.matmul(W3.T, delta3) #Calcula o erro\n",
    "        delta2 = (v2 > 0) * e2 #Calculado o delta em cima do erro\n",
    "        \n",
    "        #Primeiro neurônio\n",
    "        e1 = np.matmul(W2.T, delta2) #Calcula o erro\n",
    "        delta1 = (v1 > 0) * e1 #Calculado o delta em cima do erro\n",
    "        \n",
    "        #Quarto neurônio\n",
    "        dW4 = alpha * delta * y3.T #Calcula a correção do peso\n",
    "        W4  = W4 + dW4 #Acumula a correção corrigindo o peso\n",
    "        \n",
    "        #Terceiro neurônio\n",
    "        dW3 = alpha * delta3 * y2.T #Calcula a correção do peso\n",
    "        W3  = W3 + dW3 #Acumula a correção corrigindo o peso\n",
    "        \n",
    "        #Segundo neurônio\n",
    "        dW2 = alpha * delta2 * y1.T #Calcula a correção do peso\n",
    "        W2  = W2 + dW2 #Acumula a correção corrigindo o peso\n",
    "        \n",
    "        #Primeiro neurônio\n",
    "        dW1 = alpha * delta1 * x.T #Calcula a correção do peso\n",
    "        W1  = W1 + dW1 #Acumula a correção corrigindo o peso\n",
    "    \n",
    "    return W1, W2, W3, W4 #Retorna os pesos calculados\n",
    "\n",
    "def TestDeepReLU():   \n",
    "    \n",
    "    #Inicializa a matriz de testes zerada\n",
    "    X = np.zeros((5, 5, 5))\n",
    "    \n",
    "    #Inicializa a matriz com o número 1\n",
    "    X[:, :, 0] = [[0,1,1,0,0],\n",
    "                  [0,0,1,0,0],\n",
    "                  [0,0,1,0,0],\n",
    "                  [0,0,1,0,0],\n",
    "                  [0,1,1,1,0]]\n",
    "    \n",
    "    #Inicializa a matriz com o número 2\n",
    "    X[:, :, 1] = [[1,1,1,1,0],\n",
    "                  [0,0,0,0,1],\n",
    "                  [0,1,1,1,0],\n",
    "                  [1,0,0,0,0],\n",
    "                  [1,1,1,1,1]]\n",
    "    \n",
    "    #Inicializa a matriz com o número 3\n",
    "    X[:, :, 2] = [[1,1,1,1,0],\n",
    "                  [0,0,0,0,1],\n",
    "                  [0,1,1,1,0],\n",
    "                  [0,0,0,0,1],\n",
    "                  [1,1,1,1,0]]\n",
    "    \n",
    "    #Inicializa a matriz com o número 4\n",
    "    X[:, :, 3] = [[0,0,0,1,0],\n",
    "                  [0,0,1,1,0],\n",
    "                  [0,1,0,1,0],\n",
    "                  [1,1,1,1,1],\n",
    "                  [0,0,0,1,0]]\n",
    "    \n",
    "    #Inicializa a matriz com o número 5\n",
    "    X[:, :, 4] = [[1,1,1,1,1],\n",
    "                  [1,0,0,0,0],\n",
    "                  [1,1,1,1,0],\n",
    "                  [0,0,0,0,1],\n",
    "                  [1,1,1,1,0]]\n",
    "    \n",
    "    #Matriz de rsultados espeados\n",
    "    D = np.array([[[1,0,0,0,0]],\n",
    "                  [[0,1,0,0,0]],\n",
    "                  [[0,0,1,0,0]],\n",
    "                  [[0,0,0,1,0]],\n",
    "                  [[0,0,0,0,1]]])\n",
    "    \n",
    "    # Inicializa um valor aleatório para os pesos\n",
    "    W1 = 2*np.random.random((20, 25)) - 1\n",
    "    W2 = 2*np.random.random((20, 20)) - 1\n",
    "    W3 = 2*np.random.random((20, 20)) - 1\n",
    "    W4 = 2*np.random.random(( 5, 20)) - 1\n",
    "    \n",
    "    #Inicia o aprendizado\n",
    "    for _epoch in range(10000):\n",
    "        W1, W2, W3, W4 = DeepReLU(W1, W2, W3, W4, X, D)\n",
    "        \n",
    "    N = 5\n",
    "    for k in range(N):\n",
    "        x  = np.reshape(X[:, :, k], (25, 1)) #Transforma a matriz em um vetor 25x1\n",
    "      \n",
    "        #Primeiro neurônio\n",
    "        v1 = np.matmul(W1, x) #Multiplica a matriz W1 (pesos) com a matriz de entrada\n",
    "        y1 = ReLU(v1) #Calcula a saída do neurônio pela função ReLu\n",
    "        \n",
    "        #Segundo neurônio\n",
    "        v2 = np.matmul(W2, y1) #Multiplica a matriz W2 (pesos) com o resultado do neurônio anterior\n",
    "        y2 = ReLU(v2) #Calcula a saída do neurônio pela função ReLu\n",
    "        \n",
    "        #Terceiro neurônio\n",
    "        v3 = np.matmul(W3, y2) #Multiplica a matriz W3 (pesos) com o resultado do neurônio anterior\n",
    "        y3 = ReLU(v3) #Calcula a saída do neurônio pela função ReLu\n",
    "        \n",
    "        #Quarto neurônio\n",
    "        v = np.matmul(W4, y3) #Multiplica a matriz W4 (pesos) com o resultado do neurônio anterior\n",
    "        y = Softmax(v) #Cálcula a saída do neurônio pela função softmax\n",
    "            \n",
    "        print(\"Y = \", k+1 , \": \")\n",
    "        print(y) #Exibe os resultados\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    TestDeepReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00955ba",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "- Utiliza a função sigmoid para ativação dos nós. Esse código é utilizado para mostrar como é feito o desligamentos dos nós que não serão treinados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6da2f056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y =  1 : \n",
      "[[9.99782295e-01]\n",
      " [7.15667518e-05]\n",
      " [3.22931091e-06]\n",
      " [1.42897628e-04]\n",
      " [1.17689666e-08]]\n",
      "Y =  2 : \n",
      "[[3.54015106e-05]\n",
      " [9.97595922e-01]\n",
      " [2.36855548e-03]\n",
      " [1.20760919e-07]\n",
      " [1.21925434e-10]]\n",
      "Y =  3 : \n",
      "[[2.13503009e-05]\n",
      " [1.64924398e-03]\n",
      " [9.96828414e-01]\n",
      " [3.54080987e-05]\n",
      " [1.46558382e-03]]\n",
      "Y =  4 : \n",
      "[[7.56335730e-05]\n",
      " [1.13453829e-07]\n",
      " [3.83705311e-06]\n",
      " [9.99918213e-01]\n",
      " [2.20248083e-06]]\n",
      "Y =  5 : \n",
      "[[9.90919935e-08]\n",
      " [1.11951963e-09]\n",
      " [3.55786315e-03]\n",
      " [1.55403981e-04]\n",
      " [9.96286633e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Implementaçao da função sigmoid\n",
    "def Sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "#Implementaçao da função softmax\n",
    "def Softmax(x):\n",
    "    x  = np.subtract(x, np.max(x))\n",
    "    ex = np.exp(x)\n",
    "    \n",
    "    return ex / np.sum(ex)\n",
    "\n",
    "#Implementação da função Dropout (Sortear os neurônios a ser treinados)\n",
    "def Dropout(y, ratio):\n",
    "    ym = np.zeros_like(y)\n",
    "    \n",
    "    num = round(y.size*(1-ratio))\n",
    "    idx = np.random.choice(y.size, num, replace=False)\n",
    "    ym[idx] = 1.0 / (1.0 - ratio)\n",
    "    \n",
    "    return ym\n",
    "\n",
    "\n",
    "def DeepDropout(W1, W2, W3, W4, X, D):\n",
    "    \n",
    "    #Taxa de aprendizado\n",
    "    alpha = 0.01\n",
    "    \n",
    "    N = 5\n",
    "    #Percorre as matrizes de teste\n",
    "    for k in range(N):\n",
    "        x = np.reshape(X[:, :, k], (25, 1)) #Transforma a matiz em um vetor 25x1\n",
    "        d     = D[k,:].T #Tanspoem a matriz de resultados\n",
    "       \n",
    "        #Primeiro nreurônio\n",
    "        v1 = np.matmul(W1, x) #Multiplica W1 (peso) com o array de teste\n",
    "        y1 = Sigmoid(v1) #Calcula a saída do neurônio (função de ativação sigmoid)\n",
    "        y1 = y1 * Dropout(y1, 0.2) #Multiplica pela matriz de ativação do neurônio\n",
    "        \n",
    "        #Segundo neurônio\n",
    "        v2 = np.matmul(W2, y1) #Multiplica W2 (peso) com o array gerado pelo primeiro neurônio\n",
    "        y2 = Sigmoid(v2) #Calcula a saída do neurônio (função de ativação sigmoid)\n",
    "        y2 = y2 * Dropout(y2, 0.2) #Multiplica pela matriz de ativação do neurônio\n",
    "        \n",
    "        #Terceiro neurônio\n",
    "        v3 = np.matmul(W3, y2) #Multiplica W3 (peso) com o array gerado pelo segundo neurônio\n",
    "        y3 = Sigmoid(v3) #Calcula a saída do neurônio (função de ativação sigmoid)\n",
    "        y3 = y3 * Dropout(y3, 0.2) #Multiplica pela matriz de ativação do neurônio\n",
    "        \n",
    "        #Quarto neurônio\n",
    "        v = np.matmul(W4, y3) #Multiplica W4 (peso) com o array gerado pelo terceiro neurônio\n",
    "        y = Softmax(v) #Calcula a saída do neurônio (função de ativação softmax)\n",
    "        \n",
    "        #Quarto neurônio\n",
    "        e = d-y #Calcula o erro\n",
    "        delta = e #Calculado o delta em cima do erro\n",
    "        \n",
    "        #Terceiro neurônio\n",
    "        e3 = np.matmul(W4.T, delta) #Calcula o erro\n",
    "        delta3 = y3*(1-y3) * e3 #Calculado o delta em cima do erro\n",
    "        \n",
    "        #Segundo neurônio\n",
    "        e2 = np.matmul(W3.T, delta3) #Calcula o erro\n",
    "        delta2 = y2*(1-y2) * e2 #Calculado o delta em cima do erro\n",
    "        \n",
    "        #Primeiro neurônio\n",
    "        e1 = np.matmul(W2.T, delta2) #Calcula o erro\n",
    "        delta1 = y1*(1-y1) * e1 #Calculado o delta em cima do erro\n",
    "        \n",
    "        #Quarto neurônio\n",
    "        dW4 = alpha * delta * y3.T #Calcula a correção do peso\n",
    "        W4  = W4 + dW4 #Acumula a correção corrigindo o peso\n",
    "         \n",
    "        #Terceiro neurônio\n",
    "        dW3 = alpha * delta3 * y2.T #Calcula a correção do peso\n",
    "        W3  = W3 + dW3 #Acumula a correção corrigindo o peso\n",
    "        \n",
    "        #Segundo neurônio\n",
    "        dW2 = alpha * delta2 * y1.T #Calcula a correção do peso\n",
    "        W2  = W2 + dW2 #Acumula a correção corrigindo o peso\n",
    "        \n",
    "        #Primeiro neurônio\n",
    "        dW1 = alpha * delta1 * x.T #Calcula a correção do peso\n",
    "        W1  = W1 + dW1 #Acumula a correção corrigindo o peso\n",
    "    \n",
    "    return W1, W2, W3, W4\n",
    "\n",
    "def TestDeepDropout():\n",
    "    \n",
    "    #Inicializa a matriz com zeros\n",
    "    X = np.zeros((5, 5, 5))\n",
    "    \n",
    "    #Inicializa a matriz com o número 1\n",
    "    X[:, :, 0] = [[0,1,1,0,0],\n",
    "                  [0,0,1,0,0],\n",
    "                  [0,0,1,0,0],\n",
    "                  [0,0,1,0,0],\n",
    "                  [0,1,1,1,0]]\n",
    "    \n",
    "    #Inicializa a matriz com o número 2\n",
    "    X[:, :, 1] = [[1,1,1,1,0],\n",
    "                  [0,0,0,0,1],\n",
    "                  [0,1,1,1,0],\n",
    "                  [1,0,0,0,0],\n",
    "                  [1,1,1,1,1]]\n",
    "    \n",
    "    #Inicializa a matriz com o número 3\n",
    "    X[:, :, 2] = [[1,1,1,1,0],\n",
    "                  [0,0,0,0,1],\n",
    "                  [0,1,1,1,0],\n",
    "                  [0,0,0,0,1],\n",
    "                  [1,1,1,1,0]]\n",
    "    \n",
    "    #Inicializa a matriz com o número 4\n",
    "    X[:, :, 3] = [[0,0,0,1,0],\n",
    "                  [0,0,1,1,0],\n",
    "                  [0,1,0,1,0],\n",
    "                  [1,1,1,1,1],\n",
    "                  [0,0,0,1,0]]\n",
    "    \n",
    "    #Inicializa a matriz com o número 5\n",
    "    X[:, :, 4] = [[1,1,1,1,1],\n",
    "                  [1,0,0,0,0],\n",
    "                  [1,1,1,1,0],\n",
    "                  [0,0,0,0,1],\n",
    "                  [1,1,1,1,0]]\n",
    "    \n",
    "    #Matriz de rsultados espeados\n",
    "    D = np.array([[[1,0,0,0,0]],\n",
    "                  [[0,1,0,0,0]],\n",
    "                  [[0,0,1,0,0]],\n",
    "                  [[0,0,0,1,0]],\n",
    "                  [[0,0,0,0,1]]])\n",
    "    \n",
    "    #Inicializa um valor aleatório para os pesos\n",
    "    W1 = 2*np.random.random((20, 25)) - 1\n",
    "    W2 = 2*np.random.random((20, 20)) - 1\n",
    "    W3 = 2*np.random.random((20, 20)) - 1\n",
    "    W4 = 2*np.random.random(( 5, 20)) - 1\n",
    "    \n",
    "    #Inicia o aprendizado\n",
    "    for _epoch in range(20000):\n",
    "        W1, W2, W3, W4 = DeepDropout(W1, W2, W3, W4, X, D)\n",
    "        \n",
    "    N = 5\n",
    "    for k in range(N):\n",
    "        x  = np.reshape(X[:, :, k], (25, 1)) #Transforma a matriz em um vetor 25x1\n",
    "       \n",
    "        #Primeiro neurônio\n",
    "        v1 = np.matmul(W1, x) #Multiplica a matriz W1 (pesos) com a matriz de entrada\n",
    "        y1 = Sigmoid(v1) #Calcula a saída do neurônio pela função sigmoid\n",
    "        \n",
    "        #Segundo neurônio\n",
    "        v2 = np.matmul(W2, y1) #Multiplica a matriz W2 (pesos) com o resultado do neurônio anterior\n",
    "        y2 = Sigmoid(v2) #Calcula a saída do neurônio pela função sigmoid\n",
    "        \n",
    "        #Terceiro neurônio\n",
    "        v3 = np.matmul(W3, y2) #Multiplica a matriz W3 (pesos) com o resultado do neurônio anterior\n",
    "        y3 = Sigmoid(v3) #Calcula a saída do neurônio pela função sigmoid\n",
    "        \n",
    "        #Quarto neurônio\n",
    "        v = np.matmul(W4, y3) #Multiplica a matriz W4 (pesos) com o resultado do neurônio anterior\n",
    "        y = Softmax(v) #Calcula a saída do neurônio pela função softmax\n",
    "            \n",
    "        print(\"Y = \", k+1, \": \")\n",
    "        print(y) #Exibe os resultados\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TestDeepDropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0c3215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
